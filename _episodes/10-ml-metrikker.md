---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 10-ml-metrikker.md in _episodes_rmd/
title: "Metrikker for evalueringer af ML"
teaching: 0
exercises: 0
questions: 
- "FIXME"

objectives:
- "FIXME"

keypoints:
- "FIXME"
- "Accuracy"
- "Precision"
- "Recall"
- "Sensitivitet"
- "Specificitet"
source: Rmd
math: yes
---




## TL;DR

Metrikker for algoritmer der klassificerer binært. 

Givet: 
SP = antallet af sandt positive klassifikationer

SN = Antallet af sandt negative klassifikationer

FP = Antallet af Falsk positive klassifikationer

FN = Antallet af falsk negative klassifikationer


Accuracy: Andelen af korrekte klassifikationer.

$\frac{SP + SN}{Alle klassifikationer}$

Precision: Andelen af positive klassifikationer der er korrekte.

$\frac{SP}{SP + FP}$

Recall - aka sensitivity: Andelen af positive observationer i data der detekteres.

$\frac{SP}{SP + FN}$

Specificitet: Andelen af negative observationer i data der detekteres.

$\frac{SN}{SN + FP}$

F1-score: Et harmonisk gennemsnit af precision og recall

2 * (precision * recall)/(precision + recall)

## Cave! 

Læs det efterfølgende for blot et lille udsnit af hvad man skal overveje. 
En høj F1 score betyder ikke at modellen er god.

## Noget længere

Når vi klassificerer data, hvor godt går det så?

Vi starter med det binære tilfælde. Algoritmen forsøger at afgøre om der er en
kat på et billede. Vi giver den 1000 billeder. 

Af de 1000 billeder, er der en kat på 15 af dem. 

Algoritmen fortæller os at der er en kat på 50 af billederne, og ikke på de
resterende 950. 

Af de 50 billeder algoritmen hævder der er en kat på, er der faktisk kun en
kat på de 5. Det er de sandt positive.

De resterende 45 billeder er der ikke en kat på. De er falsk positive. 

Af de 950 billeder som algoritmen hævder der ikke er en kat på, er der faktisk
katte på de 10. De 10 er de falsk negative.

De 940 af billederne har algoritmen korrekt identificeret som værende kattefri. 
Det er de sandt negative.

Der er fire klassiske mål for hvor godt algoritmen har klaret det, defineret
som ovenfor. 

Tallene sætter vi som regel op i en "confusion" matrix:


~~~
data.frame(x = c("SP", "FP"), y = c("FN", "SN")) %>% 
  kable( align = "cc", col.names = c("", "")) %>% 
  column_spec(2, border_left = T) %>% 
  kable_styling(full_width = F)
~~~
{: .language-r}

<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;">  </th>
   <th style="text-align:center;">  </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> SP </td>
   <td style="text-align:center;border-left:1px solid;"> FN </td>
  </tr>
  <tr>
   <td style="text-align:center;"> FP </td>
   <td style="text-align:center;border-left:1px solid;"> SN </td>
  </tr>
</tbody>
</table>

Der er nogen ligheder her.

Samtlige klassifikationer = SP + FN + FP + SN

Samtlige katte, som faktisk er der = SP + FN

De billeder hvor der ikke er katte = SN + FP

Det antal gange vi siger der er en kat = SP + FP

Det antal gange vi siger der ikke er en kat = FN + TN

Med tallene ovenfor, vil det se således ud:



~~~
data.frame(gæt =c("gæt ja", "gæt nej"),x = c(5, 45), y = c(10, 940)) %>% 
  kable( align = "lcc", col.names = c("","Faktisk kat", "Ingen kat")) %>% 
  column_spec(2:3, border_left = T) %>% 
  kable_styling(full_width = F)
~~~
{: .language-r}

<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:left;">  </th>
   <th style="text-align:center;"> Faktisk kat </th>
   <th style="text-align:center;"> Ingen kat </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> gæt ja </td>
   <td style="text-align:center;border-left:1px solid;"> 5 </td>
   <td style="text-align:center;border-left:1px solid;"> 10 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gæt nej </td>
   <td style="text-align:center;border-left:1px solid;"> 45 </td>
   <td style="text-align:center;border-left:1px solid;"> 940 </td>
  </tr>
</tbody>
</table>

Accuracy er så (5 + 940) / 1000 = 0.945 eller 94.5% korrekt.

Er det godt?

Det kommer an på anvendelsen, derfor har vi de andre metrikker.

Precision: 5/(5+10) = 0.33

Recall 5/(5+45) = 0.1

Specificitet 940/(940+10) = 0.989

Er det godt? Hvis det nu ikke er katte, men brystkræft vi forsøger at opdage, så 
er det ikke nødvendigvis særlig godt. Det er kun 10% af kræfttilfældene vi fanger,
selvom vores sp

Og når vi giver diagnosen kræft, har vi kun ret i en trediedel af tilfældene. 

Det er altid nødvendigt at kigge på mere end accuracy. Og nødvendigt at overveje
hvad modellen skal bruges til. Det er særligt vigtigt i tilfælde hvor der er 
relativt få positive tilfælde i datasættet. Hvis vores algoritme konsekvent 
siger at der ikke er en kat på billedet, uanset om der er en eller ej, vil
den stadig have en accuracy på 94%

Er der en lettere måde at få de tal? Hvis vi har to faktorer med de "sande" værdier,
og de forudsagte, kan vi med funktionen `confusionMatrix` få hele møllen, og en del mere 
ud. Brug `mode = "everything"` for at få alle metrikker:


~~~
actual <- factor(rep(c(1,0), times=c(50,950)))
predicted <- factor(c(rep(1,5),rep(0,45),rep(1,10),rep(0,940)))


confusionMatrix(predicted, actual, mode = "everything", positive="1")
~~~
{: .language-r}



~~~
Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 940  45
         1  10   5
                                         
               Accuracy : 0.945          
                 95% CI : (0.929, 0.9583)
    No Information Rate : 0.95           
    P-Value [Acc > NIR] : 0.7899         
                                         
                  Kappa : 0.1339         
                                         
 Mcnemar's Test P-Value : 4.549e-06      
                                         
            Sensitivity : 0.1000         
            Specificity : 0.9895         
         Pos Pred Value : 0.3333         
         Neg Pred Value : 0.9543         
              Precision : 0.3333         
                 Recall : 0.1000         
                     F1 : 0.1538         
             Prevalence : 0.0500         
         Detection Rate : 0.0050         
   Detection Prevalence : 0.0150         
      Balanced Accuracy : 0.5447         
                                         
       'Positive' Class : 1              
                                         
~~~
{: .output}


## Hvad hvis der er mere end to klasser?

Det er samme koncept. Men bortset fra accuracy, får vi metrikkene for 
hver klasse. Her har vi tre muligheder for hvad vi ser, C, F og H, og 
algoriment forsøger at forudsige hvilken af de tre en given observation skal 
klassificeres som:

~~~
# True values
y_true <- factor(c("C","C","C","C","C","C", "F","F","F","F","F","F","F","F","F","F", "H","H","H","H","H","H","H","H","H"))
# Predicted values
y_pred <- factor(c("C","C","C","C","H","F", "C","C","C","C","C","C","H","H","F","F", "C","C","C","H","H","H","H","H","H"))

confusionMatrix(y_pred, y_true, mode="everything")
~~~
{: .language-r}



~~~
Confusion Matrix and Statistics

          Reference
Prediction C F H
         C 4 6 3
         F 1 2 0
         H 1 2 6

Overall Statistics
                                         
               Accuracy : 0.48           
                 95% CI : (0.278, 0.6869)
    No Information Rate : 0.4            
    P-Value [Acc > NIR] : 0.26772        
                                         
                  Kappa : 0.2546         
                                         
 Mcnemar's Test P-Value : 0.08689        

Statistics by Class:

                     Class: C Class: F Class: H
Sensitivity            0.6667   0.2000   0.6667
Specificity            0.5263   0.9333   0.8125
Pos Pred Value         0.3077   0.6667   0.6667
Neg Pred Value         0.8333   0.6364   0.8125
Precision              0.3077   0.6667   0.6667
Recall                 0.6667   0.2000   0.6667
F1                     0.4211   0.3077   0.6667
Prevalence             0.2400   0.4000   0.3600
Detection Rate         0.1600   0.0800   0.2400
Detection Prevalence   0.5200   0.1200   0.3600
Balanced Accuracy      0.5965   0.5667   0.7396
~~~
{: .output}

